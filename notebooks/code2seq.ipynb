{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5OLd1AL5gIQw"
   },
   "source": [
    "# A PyTorch re-implementation code for \"code2seq: Generating Sequences from Structured Representations of Code\"\n",
    "\n",
    "* Paper(Arxiv) : https://arxiv.org/abs/1808.01400  \n",
    "* Official Github : https://github.com/tech-srl/code2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnth1kMQwrNl"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import warnings\n",
    "import logging\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch\n",
    "from torch import einsum\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "\n",
    "from src import utils, messenger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = '../configs/config_code2seq.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.load(open(config_file), Loader=yaml.FullLoader)\n",
    "\n",
    "# Data source\n",
    "DATA_HOME = config['data']['home']\n",
    "TRAIN_DATA = DATA_HOME + config['data']['train']\n",
    "VALID_DATA = DATA_HOME + config['data']['valid']\n",
    "TEST_DATA  = DATA_HOME + config['data']['test']\n",
    "\n",
    "# Training parameter\n",
    "batch_size = config['training']['batch_size']\n",
    "num_epochs = config['training']['num_epochs']\n",
    "lr = config['training']['lr']\n",
    "teacher_forcing_rate = config['training']['teacher_forcing_rate']\n",
    "nesterov = config['training']['nesterov']\n",
    "weight_decay = config['training']['weight_decay']\n",
    "momentum = config['training']['momentum']\n",
    "decay_ratio = config['training']['decay_ratio']\n",
    "save_name = config['training']['save_name']\n",
    "warm_up = config['training']['warm_up']\n",
    "patience = config['training']['patience']\n",
    "\n",
    "\n",
    "\n",
    "# Model parameter\n",
    "token_size = config['model']['token_size']\n",
    "hidden_size = config['model']['hidden_size']\n",
    "num_layers = config['model']['num_layers']\n",
    "bidirectional = config['model']['bidirectional']\n",
    "rnn_dropout = config['model']['rnn_dropout']\n",
    "embeddings_dropout = config['model']['embeddings_dropout']\n",
    "num_k = config['model']['num_k']\n",
    "\n",
    "# etc\n",
    "slack_url_path = config['etc']['slack_url_path']\n",
    "info_prefix = config['etc']['info_prefix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slack_url = None\n",
    "if os.path.exists(slack_url_path):\n",
    "    slack_url = yaml.load(open(slack_url_path), Loader=yaml.FullLoader)['slack_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(1)\n",
    "random_state = 42\n",
    "\n",
    "run_id = datetime.now().strftime('%Y-%m-%d--%H-%M-%S')\n",
    "log_file = '../logs/' + run_id + '.log'\n",
    "exp_dir = '../runs/' + run_id\n",
    "os.mkdir(exp_dir)\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s | %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p', filename=log_file, level=logging.DEBUG)\n",
    "msgr = messenger.Info(info_prefix, slack_url)\n",
    "\n",
    "msgr.print_msg('run_id : {}'.format(run_id))\n",
    "msgr.print_msg('log_file : {}'.format(log_file))\n",
    "msgr.print_msg('exp_dir : {}'.format(exp_dir))\n",
    "msgr.print_msg('device : {}'.format(device))\n",
    "msgr.print_msg(str(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JM84CUHawrNv"
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = '<PAD>' \n",
    "BOS_TOKEN = '<S>' \n",
    "EOS_TOKEN = '</S>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "PAD = 0\n",
    "BOS = 1\n",
    "EOS = 2\n",
    "UNK = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjRMDX0gwrNy"
   },
   "outputs": [],
   "source": [
    "# load vocab dict\n",
    "with open(DATA_HOME + '/java-small/java-small.dict.c2s', 'rb') as file:\n",
    "    subtoken_to_count = pickle.load(file)\n",
    "    node_to_count = pickle.load(file) \n",
    "    target_to_count = pickle.load(file)\n",
    "    max_contexts = pickle.load(file)\n",
    "    num_training_examples = pickle.load(file)\n",
    "    msgr.print_msg('Dictionaries loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bRqsluoYwrOC"
   },
   "outputs": [],
   "source": [
    "# making vocab dicts for terminal subtoken, nonterminal node and target.\n",
    "\n",
    "word2id = {\n",
    "    PAD_TOKEN: PAD,\n",
    "    BOS_TOKEN: BOS,\n",
    "    EOS_TOKEN: EOS,\n",
    "    UNK_TOKEN: UNK,\n",
    "    }\n",
    "\n",
    "vocab_subtoken = utils.Vocab(word2id=word2id)\n",
    "vocab_nodes = utils.Vocab(word2id=word2id)\n",
    "vocab_target = utils.Vocab(word2id=word2id)\n",
    "\n",
    "vocab_subtoken.build_vocab(list(subtoken_to_count.keys()), min_count=0)\n",
    "vocab_nodes.build_vocab(list(node_to_count.keys()), min_count=0)\n",
    "vocab_target.build_vocab(list(target_to_count.keys()), min_count=0)\n",
    "\n",
    "vocab_size_subtoken = len(vocab_subtoken.id2word)\n",
    "vocab_size_nodes = len(vocab_nodes.id2word)\n",
    "vocab_size_target = len(vocab_target.id2word)\n",
    "\n",
    "\n",
    "msgr.print_msg('vocab_size_subtoken：' + str(vocab_size_subtoken))\n",
    "msgr.print_msg('vocab_size_nodes：' + str(vocab_size_nodes))\n",
    "msgr.print_msg('vocab_size_target：' + str(vocab_size_target))\n",
    "\n",
    "num_length_train = num_training_examples\n",
    "msgr.print_msg('num_examples : ' + str(num_length_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(object):\n",
    "\n",
    "    def __init__(self, data_path, batch_size, num_k, vocab_subtoken, vocab_nodes, vocab_target, shuffle=True, batch_time = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        data_path : path for data \n",
    "        num_examples : total lines of data file\n",
    "        batch_size : batch size\n",
    "        num_k : max ast pathes included to one examples\n",
    "        vocab_subtoken : dict of subtoken and its id\n",
    "        vocab_nodes : dict of node simbol and its id\n",
    "        vocab_target : dict of target simbol and its id\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.num_examples = self.wc(data_path)\n",
    "        self.num_k = num_k\n",
    "        \n",
    "        self.vocab_subtoken = vocab_subtoken\n",
    "        self.vocab_nodes = vocab_nodes\n",
    "        self.vocab_target = vocab_target\n",
    "        \n",
    "        self.index = 0\n",
    "        self.pointer = np.array(range(self.num_examples))\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.batch_time = batch_time\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        \n",
    "        if self.batch_time:\n",
    "            t1 = time.time()\n",
    "      \n",
    "        if self.index >= self.num_examples:\n",
    "            self.reset()\n",
    "            raise StopIteration()\n",
    "        \n",
    "        ids = self.pointer[self.index: self.index + self.batch_size]\n",
    "        seqs_S, seqs_N, seqs_E, seqs_Y = self.read_batch(ids)\n",
    "        \n",
    "        # length_k : (batch_size, k)\n",
    "        lengths_k = [len(ex) for ex in seqs_N]\n",
    "        \n",
    "        # flattening (batch_size, k, l) to (batch_size * k, l)\n",
    "        # this is useful to make torch.tensor\n",
    "        seqs_S = [symbol for k in seqs_S for symbol in k]\n",
    "        seqs_N = [symbol for k in seqs_N for symbol in k] \n",
    "        seqs_E = [symbol for k in seqs_E for symbol in k] \n",
    "        \n",
    "        # Padding\n",
    "        lengths_S = [len(s) for s in seqs_S]\n",
    "        lengths_N = [len(s) for s in seqs_N]\n",
    "        lengths_E = [len(s) for s in seqs_E]\n",
    "        lengths_Y = [len(s) for s in seqs_Y]\n",
    "        \n",
    "        max_length_S = max(lengths_S)\n",
    "        max_length_N = max(lengths_N)\n",
    "        max_length_E = max(lengths_E)\n",
    "        max_length_Y = max(lengths_Y)\n",
    "\n",
    "        padded_S = [utils.pad_seq(s, max_length_S) for s in seqs_S]\n",
    "        padded_N = [utils.pad_seq(s, max_length_N) for s in seqs_N]\n",
    "        padded_E = [utils.pad_seq(s, max_length_E) for s in seqs_E]\n",
    "        padded_Y = [utils.pad_seq(s, max_length_Y) for s in seqs_Y]\n",
    "        \n",
    "        # index for split (batch_size * k, l) into (batch_size, k, l)\n",
    "        index_N = range(len(lengths_N))\n",
    "        \n",
    "        # sort for rnn\n",
    "        seq_pairs = sorted(zip(lengths_N, index_N, padded_N, padded_S, padded_E), key=lambda p: p[0], reverse=True)\n",
    "        lengths_N, index_N, padded_N, padded_S, padded_E = zip(*seq_pairs)\n",
    "        \n",
    "        batch_S = torch.tensor(padded_S, dtype=torch.long, device=device)\n",
    "        batch_E = torch.tensor(padded_E, dtype=torch.long, device=device)\n",
    "        \n",
    "        # transpose for rnn\n",
    "        batch_N = torch.tensor(padded_N, dtype=torch.long, device=device).transpose(0, 1)\n",
    "        batch_Y = torch.tensor(padded_Y, dtype=torch.long, device=device).transpose(0, 1)\n",
    "        \n",
    "        # update index\n",
    "        self.index += self.batch_size\n",
    "        \n",
    "        if self.batch_time:\n",
    "            t2 = time.time()\n",
    "            elapsed_time = t2-t1\n",
    "            print(f\"batching time：{elapsed_time}\")\n",
    "\n",
    "        return batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        if self.shuffle:\n",
    "            self.pointer = shuffle(self.pointer)\n",
    "        self.index = 0 \n",
    "    \n",
    "    def wc(self, path):\n",
    "        wc = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for i, line in enumerate(f) :\n",
    "                wc = i\n",
    "        return wc\n",
    "        \n",
    "    def read_batch(self, ids):\n",
    "        \n",
    "        seqs_S = []\n",
    "        seqs_E = []\n",
    "        seqs_N = []\n",
    "        seqs_Y = []\n",
    "        \n",
    "        with open(self.data_path, 'r') as f:\n",
    "            for i, line in enumerate(f) :\n",
    "                if i in ids:\n",
    "                    \n",
    "                    seq_S = []\n",
    "                    seq_N = []\n",
    "                    seq_E = []\n",
    "\n",
    "                    target, *syntax_path = line.split(' ')\n",
    "                    target = target.split('|')\n",
    "                    target = utils.sentence_to_ids(self.vocab_target, target)\n",
    "                    \n",
    "                    # remove '' and '\\n' in sequence, java-small dataset contains many '' in a line.\n",
    "                    syntax_path = [s for s in syntax_path if s != '' and s != '\\n']\n",
    "\n",
    "                    # if the amount of ast path exceed the k,\n",
    "                    # uniformly sample ast pathes, as described in the paper.\n",
    "                    if len(syntax_path) > self.num_k:\n",
    "                        sampled_path_index = random.sample(range(len(syntax_path)) , self.num_k)\n",
    "                    else :\n",
    "                        sampled_path_index = range(len(syntax_path))\n",
    "                    \n",
    "                    for j in sampled_path_index:\n",
    "                        terminal1, ast_path, terminal2 = syntax_path[j].split(',')\n",
    "\n",
    "                        terminal1 = utils.sentence_to_ids(self.vocab_subtoken, terminal1.split('|'))\n",
    "                        ast_path = utils.sentence_to_ids(self.vocab_nodes, ast_path.split('|'))\n",
    "                        terminal2 = utils.sentence_to_ids(self.vocab_subtoken, terminal2.split('|')) \n",
    "\n",
    "                        seq_S.append(terminal1)\n",
    "                        seq_E.append(terminal2)\n",
    "                        seq_N.append(ast_path)\n",
    "\n",
    "                    seqs_S.append(seq_S)\n",
    "                    seqs_E.append(seq_E)\n",
    "                    seqs_N.append(seq_N)\n",
    "                    seqs_Y.append(target)\n",
    "\n",
    "        return seqs_S, seqs_N, seqs_E, seqs_Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7lLgR9WwrPS"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size_subtoken, input_size_node, token_size, hidden_size, bidirectional = True, rnn_dropout = 0.5, embeddings_dropout = 0.25):\n",
    "        \n",
    "        \"\"\"\n",
    "        input_size_subtoken : # of unique subtoken\n",
    "        input_size_node : # of unique node symbol\n",
    "        token_size : embedded token size\n",
    "        hidden_size : size of initial state of decoder\n",
    "        rnn_dropout = 0.5 : rnn drop out ratio\n",
    "        embeddings_dropout = 0.25 : dropout ratio for context vector\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.token_size = token_size\n",
    "\n",
    "        self.embedding_subtoken = nn.Embedding(input_size_subtoken, token_size, padding_idx=PAD)\n",
    "        self.embedding_node = nn.Embedding(input_size_node, token_size, padding_idx=PAD)\n",
    "        \n",
    "        self.lstm = nn.LSTM(token_size, token_size, bidirectional=bidirectional, dropout=rnn_dropout)\n",
    "        self.out = nn.Linear(token_size * 4, hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(embeddings_dropout)\n",
    "\n",
    "    def forward(self, batch_S, batch_N, batch_E, lengths_k, index_N, hidden=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_S : (B * k, l) start terminals' subtoken of each ast path\n",
    "        batch_N : (l, B*k) nonterminals' nodes of each ast path\n",
    "        batch_E : (B * k, l) end terminals' subtoken of each ast path\n",
    "        \n",
    "        lengths_k : length of k in each example\n",
    "        index_N : index for unsorting,\n",
    "        \"\"\"\n",
    "        \n",
    "        output_bag = []\n",
    "        hidden_batch = []\n",
    "        \n",
    "        \n",
    "        # (B * k, l, d)\n",
    "        encode_S = self.embedding_subtoken(batch_S)\n",
    "        encode_E = self.embedding_subtoken(batch_E)\n",
    "        \n",
    "        \n",
    "        # encode_S (B * k, d) token_representation of each ast path\n",
    "        encode_S = encode_S.sum(1)\n",
    "        encode_E = encode_E.sum(1)\n",
    "        \n",
    "        #print('encode_S', encode_S)\n",
    "        #print('encode_E', encode_E)\n",
    "        \n",
    "        \n",
    "        # emb_N :(l, B*k, d)\n",
    "        \n",
    "        emb_N = self.embedding_node(batch_N)\n",
    "        packed = pack_padded_sequence(emb_N, lengths_N)\n",
    "        output, _ = self.lstm(packed, hidden)\n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        \n",
    "        # output of shape (seq_len, B * k, num_directions * d)\n",
    "        # -> (B * k, seq_len, num_directions * d)\n",
    "        output = output.transpose(0, 1)\n",
    "        \n",
    "        #For the unpacked case, the directions can be separated using \n",
    "        # output.view(seq_len, batch, num_directions, hidden_size), \n",
    "        # with forward and backward being direction 0 and 1 respectively.\n",
    "        # -> (B * k, seq_len, num_directions,  d)\n",
    "        output = output.view(batch_N.shape[1], batch_N.shape[0], 2, self.token_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #a b c d e\n",
    "        #f g h 0 0\n",
    "        #i j 0 0 0\n",
    "        #k 0 0 0 0\n",
    "        # output_normal should be [e, h, j, k] \n",
    "        # output_reverse          [a, f, i, k]\n",
    "        \n",
    "        ln = [lengths_N[0] * i + (l - 1) for i,l in enumerate(lengths_N)]\n",
    "        output_normal = output[:, :,  0, :]\n",
    "        output_normal = output_normal.contiguous().view(-1, self.token_size)\n",
    "        output_normal = output_normal[ln]\n",
    "        \n",
    "        output_reverse = output[:,  0,  1, :].view(batch_N.shape[1], self.token_size)\n",
    "        \n",
    "        # encode_N  :(B*k, 2d)\n",
    "        encode_N = torch.cat([output_normal, output_reverse], dim=1)\n",
    "        \n",
    "        # encode_SNE  : (B*k, 4d)\n",
    "        encode_SNE = torch.cat([encode_N, encode_S, encode_E], dim=1)\n",
    "        \n",
    "        # encode_SNE  : (B*k, d)\n",
    "        encode_SNE = self.out(encode_SNE)\n",
    "        \n",
    "        # unsort as example\n",
    "        index = torch.tensor(index_N, dtype=torch.long, device=device)\n",
    "        encode_SNE = torch.index_select(encode_SNE, dim=0, index=index)\n",
    "        \n",
    "        # as is in  https://github.com/tech-srl/code2seq/blob/ec0ae309efba815a6ee8af88301479888b20daa9/model.py#L511\n",
    "        encode_SNE = self.dropout(encode_SNE)\n",
    "        \n",
    "        # output_bag  : [ B, (k, d) ]\n",
    "        output_bag = torch.split(encode_SNE, lengths_k, dim=0)\n",
    "        # hidden_0  : [ B, (d) ]\n",
    "        hidden_0 = [ob.mean(0).unsqueeze(dim=0) for ob in output_bag]\n",
    "\n",
    "        # hidden_0  : (1, B, d)\n",
    "        # size should be like (1, batch_size, hidden_size)\n",
    "        hidden_0 = torch.cat(hidden_0, dim=0).unsqueeze(dim=0)\n",
    "        \n",
    "        return output_bag, hidden_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "flU2AatIwrPl"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, rnn_dropout):\n",
    "        \"\"\"\n",
    "        hidden_size : decoder unit size, \n",
    "        output_size : decoder output size, \n",
    "        rnn_dropout : dropout ratio for rnn\n",
    "        \"\"\"\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, dropout=rnn_dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, seqs, hidden):\n",
    "        \n",
    "        emb = self.embedding(seqs)\n",
    "        output, hidden = self.gru(emb, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XCPLR4MkQ_U1"
   },
   "source": [
    "#### memo. Attention and its score function\n",
    "\n",
    "In this imprementation, score function is:\n",
    "$$\n",
    "   \\mathrm{score}(\\bar{h}_s, h_t) = h_t^{\\mathrm{T}} W_a \\bar{h}_s .\n",
    "$$\n",
    "\n",
    "The weight is\n",
    "$$\n",
    "    a_t(s) = \\frac{\\exp(\\mathrm{score}(\\bar{h}_s, h_t))}{\\sum^S_{s'=1}\\exp(\\mathrm{score}(\\bar{h}_s, h_t))} .\n",
    "$$\n",
    "\n",
    "And so on...\n",
    "$$\n",
    "    c_t = \\sum^S_{s=1} a_t(s) \\bar{h}_s\n",
    "$$\n",
    "$$\n",
    "    \\tilde{h}_t = \\tanh(W_h h_t + W_c c_t + b)\n",
    "$$\n",
    "$$\n",
    "    y_t = \\mathrm{softmax}(W_{out}\\tilde{h}_t + b_{out})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zsuAuSteQ_U3"
   },
   "outputs": [],
   "source": [
    "class EncoderDecoder_with_Attention(nn.Module):\n",
    "    \n",
    "    \"\"\"Conbine Encoder and Decoder\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size_subtoken, input_size_node, token_size, output_size, hidden_size, bidirectional = True, rnn_dropout = 0.5, embeddings_dropout = 0.25):\n",
    "\n",
    "        super(EncoderDecoder_with_Attention, self).__init__()\n",
    "        self.encoder = Encoder(input_size_subtoken, input_size_node, token_size, hidden_size, bidirectional, rnn_dropout, embeddings_dropout)\n",
    "        self.decoder = Decoder(hidden_size, output_size, rnn_dropout)\n",
    "        \n",
    "        self.W_a  = torch.rand((hidden_size, hidden_size), dtype=torch.float,device=device , requires_grad=True)\n",
    "        self.W_cc = torch.rand((hidden_size, hidden_size), dtype=torch.float,device=device , requires_grad=True)\n",
    "        self.W_ch = torch.rand((hidden_size, hidden_size), dtype=torch.float,device=device , requires_grad=True)\n",
    "        self.b    = torch.rand(hidden_size, dtype=torch.float, device=device, requires_grad=True)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.W_a)\n",
    "        nn.init.xavier_uniform_(self.W_cc)\n",
    "        nn.init.xavier_uniform_(self.W_ch)\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, batch_S, batch_N, batch_E, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S, max_length_N,max_length_E,max_length_Y, lengths_k, index_N, terget_max_length, batch_Y=None, use_teacher_forcing=False):\n",
    "\n",
    "        # Encoder\n",
    "        encoder_output_bag, encoder_hidden = \\\n",
    "        self.encoder(batch_S, batch_N, batch_E, lengths_k, index_N)\n",
    "        _batch_size = len(encoder_output_bag)\n",
    "        \n",
    "        # calc initial decoder state with attention\n",
    "        decoder_hidden = self.attention(encoder_output_bag, encoder_hidden, lengths_k)\n",
    "        \n",
    "        # make initial input for decoder\n",
    "        decoder_input = torch.tensor([BOS] * _batch_size, dtype=torch.long, device=device)\n",
    "        decoder_input = decoder_input.unsqueeze(0)  # (1, batch_size)\n",
    "        \n",
    "        # output holder\n",
    "        decoder_outputs = torch.zeros(terget_max_length, _batch_size, self.decoder.output_size, device=device)\n",
    "        \n",
    "        for t in range(terget_max_length):\n",
    "            \n",
    "            # Decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            # calc next hidden state w/ attention\n",
    "            decoder_hidden = self.attention(encoder_output_bag, decoder_hidden, lengths_k)\n",
    "            \n",
    "            # hold output\n",
    "            decoder_outputs[t] = decoder_output\n",
    "            \n",
    "            # Teacher Forcing\n",
    "            if use_teacher_forcing and batch_Y is not None:\n",
    "                decoder_input = batch_Y[t].unsqueeze(0)\n",
    "            else: \n",
    "                decoder_input = decoder_output.max(-1)[1]\n",
    "        \n",
    "        return decoder_outputs\n",
    "    \n",
    "    def attention(self, encoder_output_bag, hidden, lengths_k):\n",
    "        \n",
    "        \"\"\"\n",
    "        encoder_output_bag : (batch * k, hidden_size) bag of embedded ast path\n",
    "        hidden : (batch, hidden_size)  previous hidden state\n",
    "        lengths_k : (batch, 1) length of k in each example\n",
    "        \"\"\"\n",
    "        \n",
    "        # d_hidden:(1, sample, hidden_size) -> (sample, hidden_size)\n",
    "        d_hidden = hidden.squeeze(0)\n",
    "        \n",
    "        # d_hidden:(sample, hidden_size) \n",
    "        # -> (sample * k, hidden_size)\n",
    "        \n",
    "        index = torch.cat([ torch.tensor([i] * k, dtype=torch.long, device=device) for i,k in enumerate(lengths_k) ], dim =0)\n",
    "        d_hidden = torch.index_select(d_hidden, dim=0, index=index)\n",
    "        \n",
    "        # e_output : (sample * k, hidden_size)\n",
    "        e_output = torch.cat(encoder_output_bag, dim=0)\n",
    "        \n",
    "        # e_output: [sample * num_k(i), hidden_size(j)]\n",
    "        # self.W_a  : [hidden_size(j), hidden_size(k)]\n",
    "        # -> : [sample * num_k(i), hidden_size(k)]\n",
    "        score = einsum('ij,jk->ik', e_output, self.W_a)\n",
    "        \n",
    "        # d_hidden: [sample * k(i), hidden_size(j)]\n",
    "        # score:    [sample * k(i), hidden_size(j)]\n",
    "        # -> score: [sample * k(i), 1]\n",
    "        \n",
    "        score = torch.einsum('ij,ij->i', d_hidden, score).unsqueeze(1)\n",
    "        \n",
    "        # score: [sample * k(i), 1]\n",
    "        # -> [sample, k, 1]\n",
    "        score = torch.split(score, lengths_k, dim=0)\n",
    "        \n",
    "        #  attn_weights: [sample, k, 1]\n",
    "        attn_weights = [F.softmax(s, dim=0) for s in score]\n",
    "        \n",
    "        # aw: [k(i), 1(j)]\n",
    "        # eo: [k(i), hidden_size(k)]\n",
    "        # -> [1(j), hidden_size(k)]\n",
    "        context_vector = [torch.einsum('ij,ik->jk', aw, eo) for aw, eo in zip(attn_weights, encoder_output_bag)]\n",
    "        context_vector = torch.cat(context_vector, dim=0)\n",
    "        \n",
    "        # context_vector : (sample(i), hidden_size(j))\n",
    "        # self.W_cc : (hidden_size(j), hidden_size(k))\n",
    "        # -> (sample(i), hidden_size(k))\n",
    "        \n",
    "        # hidden : (1(i), sample(j), hidden_size(k))\n",
    "        # self.W_ch : (hidden_size(k), hidden_size(l))\n",
    "        # -> (sample(j), hidden_size(l))\n",
    "        \n",
    "        # decoder_hidden : (sample, hidden_size)\n",
    "        decoder_hidden = F.tanh(torch.einsum('ij,jk->ik', context_vector, self.W_cc) + \n",
    "                                torch.einsum('ijk,kl->jl', hidden, self.W_ch) + \n",
    "                                self.b)\n",
    "        \n",
    "        # decoder_hidden : (1, sample, hidden_size)\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(0)\n",
    "        \n",
    "        return decoder_hidden\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YqnxqyXqwrP0"
   },
   "outputs": [],
   "source": [
    "mce = nn.CrossEntropyLoss(size_average=False, ignore_index=PAD)\n",
    "def masked_cross_entropy(logits, target):\n",
    "    return mce(logits.view(-1, logits.size(-1)), target.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kp7px_O7-1J"
   },
   "outputs": [],
   "source": [
    "batch_time = False\n",
    "train_dataloader = DataLoader(TRAIN_DATA, batch_size, num_k, vocab_subtoken, vocab_nodes, vocab_target, batch_time=batch_time, shuffle=True)\n",
    "valid_dataloader = DataLoader(VALID_DATA, batch_size, num_k, vocab_subtoken, vocab_nodes, vocab_target, shuffle=False)\n",
    "\n",
    "model_args = {\n",
    "    'input_size_subtoken' : vocab_size_subtoken,\n",
    "    'input_size_node' : vocab_size_nodes,\n",
    "    'output_size' : vocab_size_target,\n",
    "    'hidden_size' : hidden_size, \n",
    "    'token_size' : token_size,\n",
    "    'bidirectional' : bidirectional,\n",
    "    'rnn_dropout' : rnn_dropout, \n",
    "    'embeddings_dropout' : embeddings_dropout\n",
    "}\n",
    "\n",
    "model = EncoderDecoder_with_Attention(**model_args).to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum, nesterov = nesterov)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda = lambda epoch: decay_ratio ** epoch)\n",
    "\n",
    "fname = exp_dir + save_name\n",
    "early_stopping = utils.EarlyStopping(fname, patience, warm_up, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IujU0wrrwrQE"
   },
   "outputs": [],
   "source": [
    "def compute_loss(batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N, model, optimizer=None, is_train=True):\n",
    "    model.train(is_train)\n",
    "    \n",
    "    use_teacher_forcing = is_train and (random.random() < teacher_forcing_rate)\n",
    "    \n",
    "    \n",
    "    target_max_length = batch_Y.size(0)\n",
    "    pred_Y = model(batch_S, batch_N, batch_E, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N, target_max_length, batch_Y, use_teacher_forcing)\n",
    "    \n",
    "    loss = masked_cross_entropy(pred_Y.contiguous(), batch_Y.contiguous())\n",
    "    \n",
    "    if is_train:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    batch_Y = batch_Y.transpose(0, 1).contiguous().data.cpu().tolist()\n",
    "    pred = pred_Y.max(dim=-1)[1].data.cpu().numpy().T.tolist()\n",
    "    \n",
    "    \n",
    "    return loss.item(), batch_Y, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nTCOrAinwrQQ"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Training Loop\n",
    "# \n",
    "progress_bar = False # progress bar is visible in progress_bar = False\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs+1):\n",
    "    train_loss = 0.\n",
    "    train_refs = []\n",
    "    train_hyps = []\n",
    "    valid_loss = 0.\n",
    "    valid_refs = []\n",
    "    valid_hyps = []\n",
    "    \n",
    "    # train\n",
    "    for batch in tqdm(train_dataloader, total=train_dataloader.num_examples // train_dataloader.batch_size , desc='TRAIN'):\n",
    "        batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S, max_length_N,max_length_E,max_length_Y, lengths_k, index_N = batch\n",
    "        \n",
    "        loss, gold, pred = compute_loss(\n",
    "            batch_S, batch_N, batch_E, batch_Y, \n",
    "            lengths_S, lengths_N, lengths_E, lengths_Y, \n",
    "            max_length_S,max_length_N,max_length_E,max_length_Y, \n",
    "            lengths_k, index_N, model, optimizer,\n",
    "            is_train=True\n",
    "            )\n",
    "        \n",
    "        train_loss += loss\n",
    "        train_refs += gold\n",
    "        train_hyps += pred\n",
    "    \n",
    "    # valid\n",
    "    for batch in tqdm(valid_dataloader, total=valid_dataloader.num_examples // valid_dataloader.batch_size , desc='VALID'):\n",
    "\n",
    "        batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N = batch\n",
    "\n",
    "        loss, gold, pred = compute_loss(\n",
    "            batch_S, batch_N, batch_E, batch_Y, \n",
    "            lengths_S, lengths_N, lengths_E, lengths_Y, \n",
    "            max_length_S,max_length_N,max_length_E,max_length_Y, \n",
    "            lengths_k, index_N, model, optimizer,\n",
    "            is_train=False\n",
    "            )\n",
    "        \n",
    "        valid_loss += loss\n",
    "        valid_refs += gold\n",
    "        valid_hyps += pred\n",
    "            \n",
    "\n",
    "    train_loss = np.sum(train_loss) / train_dataloader.num_examples\n",
    "    valid_loss = np.sum(valid_loss) / valid_dataloader.num_examples\n",
    "    \n",
    "    # BLEU\n",
    "    train_bleu = utils.calc_bleu(train_refs, train_hyps)\n",
    "    valid_bleu = utils.calc_bleu(valid_refs, valid_hyps)\n",
    "    \n",
    "    # F1 etc\n",
    "    train_precision, train_recall, train_f1 = utils.calculate_results_set(train_refs, train_hyps)\n",
    "    valid_precision, valid_recall, valid_f1 = utils.calculate_results_set(valid_refs, valid_hyps)\n",
    "\n",
    "    \n",
    "    early_stopping(valid_f1, model, epoch)\n",
    "    if early_stopping.early_stop:\n",
    "        msgr.print_msg(\"Early stopping\")\n",
    "        break\n",
    "    \n",
    "    \n",
    "    msgr.print_msg('Epoch {}: train_loss: {:5.2f}  train_bleu: {:2.4f}  train_f1: {:2.4f}  valid_loss: {:5.2f}  valid_bleu: {:2.4f}  valid_f1: {:2.4f}'.format(\n",
    "            epoch, train_loss, train_bleu, train_f1, valid_loss, valid_bleu, valid_f1))\n",
    "    \n",
    "    \n",
    "    print('-'*80)\n",
    "    \n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QsQay14VEwYJ"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swYlOn13Q_VU"
   },
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    'input_size_subtoken' : vocab_size_subtoken,\n",
    "    'input_size_node' : vocab_size_nodes,\n",
    "    'output_size' : vocab_size_target,\n",
    "    'hidden_size' : hidden_size, \n",
    "    'token_size' : token_size,\n",
    "    'bidirectional' : bidirectional,\n",
    "    'rnn_dropout' : rnn_dropout, \n",
    "    'embeddings_dropout' : embeddings_dropout\n",
    "}\n",
    "\n",
    "model = EncoderDecoder_with_Attention(**model_args).to(device)\n",
    "\n",
    "fname = exp_dir + save_name\n",
    "ckpt = torch.load(fname)\n",
    "model.load_state_dict(ckpt)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2GKrKzSDQ_VW"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(TEST_DATA, batch_size, num_k, vocab_subtoken, vocab_nodes, vocab_target, batch_time=batch_time, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lY6ty-rNG76n"
   },
   "outputs": [],
   "source": [
    "refs_list = []\n",
    "hyp_list = []\n",
    "\n",
    "for batch in tqdm(test_dataloader,\n",
    "                      total=test_dataloader.num_examples // test_dataloader.batch_size + 1,\n",
    "                      desc='TEST'):\n",
    "    \n",
    "    batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N = batch\n",
    "    target_max_length = batch_Y.size(0)\n",
    "    use_teacher_forcing = False\n",
    "    \n",
    "    pred_Y = model(batch_S, batch_N, batch_E, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N, target_max_length, batch_Y, use_teacher_forcing)\n",
    "    \n",
    "    refs = batch_Y.transpose(0, 1).contiguous().data.cpu().tolist()[0]\n",
    "    pred = pred_Y.max(dim=-1)[1].data.cpu().numpy().T.tolist()[0]\n",
    "    \n",
    "    refs_list.append(refs)\n",
    "    hyp_list.append(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VVDf6TOAPE8z"
   },
   "outputs": [],
   "source": [
    "msgr.print_msg('Tested model : ' + fname)\n",
    "\n",
    "test_precision, test_recall, test_f1 = utils.calculate_results(refs_list, hyp_list)\n",
    "msgr.print_msg('Test : precision {:1.5f}, recall {:1.5f}, f1 {:1.5f}'.format(test_precision, test_recall, test_f1))\n",
    "\n",
    "test_precision, test_recall, test_f1 = utils.calculate_results_set(refs_list, hyp_list)\n",
    "msgr.print_msg('Test(set) : precision {:1.5f}, recall {:1.5f}, f1 {:1.5f}'.format(test_precision, test_recall, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_time = False\n",
    "test_dataloader = DataLoader(TEST_DATA, 1, num_k, vocab_subtoken, vocab_nodes, vocab_target, batch_time=batch_time, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "batch_S, batch_N, batch_E, batch_Y, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N = next(test_dataloader)\n",
    "\n",
    "sentence_Y = ' '.join(utils.ids_to_sentence(vocab_target, batch_Y.data.cpu().numpy()[:-1, 0]))\n",
    "msgr.print_msg('tgt: {}'.format(sentence_Y))\n",
    "\n",
    "target_max_length = batch_Y.size(0)\n",
    "use_teacher_forcing = False\n",
    "output = model(batch_S, batch_N, batch_E, lengths_S, lengths_N, lengths_E, lengths_Y, max_length_S,max_length_N,max_length_E,max_length_Y, lengths_k, index_N, target_max_length, batch_Y, use_teacher_forcing)\n",
    "\n",
    "output = output.max(dim=-1)[1].view(-1).data.cpu().tolist()\n",
    "output_sentence = ' '.join(utils.ids_to_sentence(vocab_target, utils.trim_eos(output)))\n",
    "msgr.print_msg('out: {}'.format(output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "code2seq.ipynb",
   "private_outputs": true,
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
